{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from Smart Home (https://www.iot-now.com/smart-homes-2/)\n",
      "Found 16 unique article URLs\n",
      "\n",
      "Scraping 16 articles...\n",
      "[1/16] Scraping: https://www.iot-now.com/2025/11/05/153958-quectel-launches-new-lora-v2x-wi-fi-and-5g-durable-fiberglass-antennas/ [Smart Home]\n",
      "[2/16] Scraping: https://www.iot-now.com/2025/11/05/153952-ai-driven-network-automation-the-top-priority-for-telcos-in-the-next-12-months-new-research-from-motive/ [Smart Home]\n",
      "[3/16] Scraping: https://www.iot-now.com/2025/10/22/153652-smart-home-security-enjoys-prpl-patch-at-paris-summit/ [Smart Home]\n",
      "[4/16] Scraping: https://www.iot-now.com/2025/10/22/153646-ceva-introduces-wi-fi-7-1x1-client-ip/ [Smart Home]\n",
      "[5/16] Scraping: https://www.iot-now.com/2025/08/19/152654-driving-the-new-era-of-ai-and-iot-with-powercasts-one-stop-shop-for-wireless-power-solutions/ [Smart Home]\n",
      "[6/16] Scraping: https://www.iot-now.com/2025/08/18/152636-the-number-of-home-energy-management-systems-in-europe-and-north-america-reached-4-5-million-in-2024/ [Smart Home]\n",
      "[7/16] Scraping: https://www.iot-now.com/2025/06/23/152138-introducing-the-multiverse-of-techled-world/ [Smart Home]\n",
      "[8/16] Scraping: https://www.iot-now.com/2025/05/18/151614-improving-iot-applications-the-critical-role-of-reliable-connectivity/ [Smart Home]\n",
      "[9/16] Scraping: https://www.iot-now.com/2025/04/28/151127-farnell-adds-nxp-semiconductors-microcontrollers-and-frdm-development-boards-to-portfolio/ [Smart Home]\n",
      "[10/16] Scraping: https://www.iot-now.com/2025/03/13/150349-mediatek-unveils-genio-720-and-genio-520-iot-platforms-for-generative-ai-applications/ [Smart Home]\n",
      "[11/16] Scraping: https://www.iot-now.com/2025/03/12/150302-synaptics-extends-edge-ai-portfolio-with-high-performance-adaptive-mcus-for-multimodal-context-aware-computing/ [Smart Home]\n",
      "[12/16] Scraping: https://www.iot-now.com/2025/03/11/150277-semtech-launches-lora-plus-lr2021-transceiver-with-lora-gen-4-technology/ [Smart Home]\n",
      "[13/16] Scraping: https://www.iot-now.com/2025/03/06/150163-stmicroelectronics-new-integrated-stm32wba6-wireless-microcontrollers-combine-extra-features-and-performance-with-power-efficiency/ [Smart Home]\n",
      "[14/16] Scraping: https://www.iot-now.com/2025/02/28/149934-arlo-and-samsung-smartthings-expand-partnership-to-enhance-home-security-with-ai-powered-features/ [Smart Home]\n",
      "[15/16] Scraping: https://www.iot-now.com/2025/02/26/149894-infineon-and-eatron-extend-collaboration-for-ai-powered-battery-management-solutions-to-industrial-and-consumer-applications/ [Smart Home]\n",
      "[16/16] Scraping: https://www.iot-now.com/2025/02/12/149591-mouser-ships-stmicroelectronics-stm32n6-edge-ai-microcontrollers-for-automotive-and-robotics/ [Smart Home]\n",
      "\n",
      "✓ Successfully scraped 16 articles\n",
      "✓ Saved results to c:\\Users\\Admin\\Documents\\VIETTEL\\2-crawl-articles\\output/iotnow_smarthome_20251124_090142.xlsx\n",
      "\n",
      "Breakdown by source:\n",
      "  - Smart Home News: 16 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import urllib3\n",
    "import os\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = \"https://www.iot-now.com/\"\n",
    "NEWS_URLS = {\n",
    "    f\"{BASE_URL}smart-homes-2/\": \"Smart Home\"\n",
    "}\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "EXCLUDE_PATHS = ['/category/', '/tag/', '/author/', '/wp-admin', '/feed', \n",
    "                 '/subscribe', '/contact', '/about-us/', '/privacy', '/terms', \n",
    "                 '?s=', '/search', '/newsletter']\n",
    "\n",
    "\n",
    "def fetch_page(url):\n",
    "    \"\"\"Fetch and parse a webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, verify=False, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_valid_article_url(url):\n",
    "    \"\"\"Check if URL is likely an article.\"\"\"\n",
    "    if not url.startswith(BASE_URL):\n",
    "        return False\n",
    "    \n",
    "    path = url[len(BASE_URL):]\n",
    "    \n",
    "    if len(path) < 10 or path == '/' or path == 'smart-homes-2/':\n",
    "        return False\n",
    "    \n",
    "    if any(excluded in path.lower() for excluded in EXCLUDE_PATHS):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_article_links():\n",
    "    \"\"\"Extract article URLs from IoT Now Smart Home pages with source tracking.\"\"\"\n",
    "    all_links = {}  # {article_url: {'source': source_name, 'date': date}}\n",
    "    \n",
    "    for news_url, source_name in NEWS_URLS.items():\n",
    "        print(f\"Fetching articles from {source_name} ({news_url})\")\n",
    "        soup = fetch_page(news_url)\n",
    "        \n",
    "        if not soup:\n",
    "            continue\n",
    "        \n",
    "        # Find article containers\n",
    "        for item in soup.find_all(\"div\", class_=\"category__post\"):\n",
    "            h2_tag = item.find(\"h2\", class_=\"category__title\")\n",
    "            if h2_tag:\n",
    "                a_tag = h2_tag.find(\"a\", href=True)\n",
    "                if a_tag:\n",
    "                    url = urljoin(BASE_URL, a_tag[\"href\"].strip())\n",
    "                    if is_valid_article_url(url):\n",
    "                        # Find the date in the item container\n",
    "                        date_tag = item.find(\"time\", class_=\"entry-date published updated\")\n",
    "                        if not date_tag:\n",
    "                            date_tag = item.find(\"time\", class_=\"entry-date\")\n",
    "                        date_text = date_tag.get_text(strip=True) if date_tag else \"\"\n",
    "                        \n",
    "                        all_links[url] = {\n",
    "                            'source': source_name,\n",
    "                            'date': date_text\n",
    "                        }\n",
    "    \n",
    "    print(f\"Found {len(all_links)} unique article URLs\")\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def scrape_article(url, source_name, date_from_listing):\n",
    "    \"\"\"Extract content from a single article.\"\"\"\n",
    "    try:\n",
    "        soup = fetch_page(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        # Find title - look for h1 on the article page\n",
    "        title = soup.find(\"h1\", class_=\"entry-title\")\n",
    "        if not title:\n",
    "            title = soup.find(\"h1\")\n",
    "        \n",
    "        # Find description/summary - look for article excerpt or meta description\n",
    "        description = soup.find(\"meta\", property=\"og:description\")\n",
    "        if description:\n",
    "            description_text = description.get(\"content\", \"\")\n",
    "        else:\n",
    "            # Try to get the first paragraph of content\n",
    "            content_div = soup.find(\"div\", class_=\"entry-content\")\n",
    "            if content_div:\n",
    "                first_p = content_div.find(\"p\")\n",
    "                description_text = first_p.get_text(strip=True) if first_p else \"\"\n",
    "            else:\n",
    "                description_text = \"\"\n",
    "        \n",
    "        # Use date from listing page, or try to find on article page as fallback\n",
    "        date_text = date_from_listing\n",
    "        if not date_text:\n",
    "            date_tag = soup.find(\"time\", class_=\"entry-date published\")\n",
    "            if not date_tag:\n",
    "                date_tag = soup.find(\"time\")\n",
    "            if date_tag:\n",
    "                date_text = date_tag.get(\"datetime\", date_tag.get_text(strip=True))\n",
    "        \n",
    "        return {\n",
    "            \"Nguồn\": \"Smart Home News\",\n",
    "            \"Tiêu đề\": title.get_text(strip=True) if title else \"\",\n",
    "            \"Mô tả\": description_text,\n",
    "            \"Ngày\": date_text,\n",
    "            \"URL\": url\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main scraping workflow.\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    article_links = get_article_links()\n",
    "    \n",
    "    print(f\"\\nScraping {len(article_links)} articles...\")\n",
    "    \n",
    "    for i, (url, info) in enumerate(article_links.items(), 1):\n",
    "        print(f\"[{i}/{len(article_links)}] Scraping: {url} [{info['source']}]\")\n",
    "        data = scrape_article(url, info['source'], info['date'])\n",
    "        if data and data[\"Tiêu đề\"]:  # Only add if title was found\n",
    "            articles.append(data)\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = r\"C:\\Users\\Admin\\Documents\\VIETTEL\\2-crawl-articles\\scraper\\output-scraper\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        excel_file = f\"{output_dir}/iotnow_smarthome_{timestamp}.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"\\n✓ Successfully scraped {len(articles)} articles\")\n",
    "        print(f\"✓ Saved results to {excel_file}\")\n",
    "        \n",
    "        # Show breakdown by source\n",
    "        print(\"\\nBreakdown by source:\")\n",
    "        for source in df[\"Nguồn\"].value_counts().items():\n",
    "            print(f\"  - {source[0]}: {source[1]} articles\")\n",
    "    else:\n",
    "        print(\"\\n✗ No articles were successfully scraped.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
