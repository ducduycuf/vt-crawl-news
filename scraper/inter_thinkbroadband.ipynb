{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from https://www.thinkbroadband.com/topics/broadband\n",
      "Found 10 article URLs\n",
      "Scraping: https://www.thinkbroadband.com/news/oopsie-for-virgin-media-website-with-404-error-on-pages-for-10-minutes\n",
      "Scraping: https://www.thinkbroadband.com/news/bduk-update-to-project-gigabit-contracts\n",
      "Scraping: https://www.thinkbroadband.com/news/weekly-brief-21st-november-2025\n",
      "Scraping: https://www.thinkbroadband.com/news/ofcom-connected-nations-report-2025-fixed-line\n",
      "Scraping: https://www.thinkbroadband.com/news/6000-homes-in-ripon-can-get-virgin-media-o2-full-fibre-service\n",
      "Scraping: https://www.thinkbroadband.com/news/totsco-approaches-2-million-ots-switches\n",
      "Scraping: https://www.thinkbroadband.com/news/ofcom-connection-nations-report-2025-mobile\n",
      "Scraping: https://www.thinkbroadband.com/news/connexin-full-fibre-network-now-on-cityfibre-platform\n",
      "Scraping: https://www.thinkbroadband.com/news/cloudflare-outage-affects-websites-globally\n",
      "Scraping: https://www.thinkbroadband.com/news/weekly-brief-14-november-2025\n",
      "\n",
      "Saved results to c:\\Users\\Admin\\Documents\\VIETTEL\\2-crawl-articles\\output/thinkbroadband_20251124_100518.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import urllib3\n",
    "import os\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = \"https://www.thinkbroadband.com/\"\n",
    "NEWS_URL = f\"{BASE_URL}topics/broadband\"\n",
    "\n",
    "EXCLUDE_PATHS = [\n",
    "    '/category/', '/tag/', '/author/', '/wp-admin', '/feed',\n",
    "    '/subscribe', '/contact', '/about', '/privacy', '/terms',\n",
    "    '?s=', '/search', '/newsletter'\n",
    "]\n",
    "\n",
    "# ---------- Selenium Setup -----------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "def fetch_page(url):\n",
    "    \"\"\"Load a webpage using Selenium and return BeautifulSoup.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1.5)  # ensure JS content loads\n",
    "    html = driver.page_source\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "def is_valid_article_url(url):\n",
    "    \"\"\"Check if URL is likely an article.\"\"\"\n",
    "    if not url.startswith(BASE_URL):\n",
    "        return False\n",
    "\n",
    "    path = url[len(BASE_URL):]\n",
    "\n",
    "    if len(path) < 10 or path == '/':\n",
    "        return False\n",
    "\n",
    "    if any(excluded in path.lower() for excluded in EXCLUDE_PATHS):\n",
    "        return False\n",
    "\n",
    "    return '-' in path\n",
    "\n",
    "\n",
    "def get_article_links():\n",
    "    print(f\"Fetching articles from {NEWS_URL}\")\n",
    "    soup = fetch_page(NEWS_URL)\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    # Find main article containers\n",
    "    containers = soup.find_all(\n",
    "        lambda tag: tag.name in (\"article\", \"div\") and tag.find([\"h2\", \"h3\"], class_=\"entry-title\")\n",
    "    )\n",
    "\n",
    "    if not containers:\n",
    "        containers = soup.find_all(\"div\", class_=\"entry\")\n",
    "\n",
    "    for item in containers:\n",
    "        h_tag = item.find([\"h2\", \"h3\"], class_=\"entry-title\")\n",
    "        a_tag = h_tag.find(\"a\", href=True) if h_tag else item.find(\"a\", href=True)\n",
    "\n",
    "        if a_tag:\n",
    "            url = urljoin(BASE_URL, a_tag[\"href\"].strip())\n",
    "            if is_valid_article_url(url):\n",
    "                links.add(url)\n",
    "\n",
    "    print(f\"Found {len(links)} article URLs\")\n",
    "    return list(links)\n",
    "\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Extract content from a single article.\"\"\"\n",
    "    try:\n",
    "        soup = fetch_page(url)\n",
    "\n",
    "        # Title\n",
    "        title_tag = soup.find(\"h1\") or soup.find(\"h2\", class_=\"entry-title\") or soup.find(\"h2\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "        # Description\n",
    "        desc_tag = (\n",
    "            soup.find(\"div\", class_=\"entry-content\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"p\", class_=\"lead\")\n",
    "        )\n",
    "        description = desc_tag.get_text(\" \", strip=True) if desc_tag else \"\"\n",
    "\n",
    "        # Date\n",
    "        date_text = \"\"\n",
    "        date_div = soup.find(\"div\", class_=\"posted-on\")\n",
    "\n",
    "        if date_div:\n",
    "            a = date_div.find(\"a\")\n",
    "            if a:\n",
    "                date_text = a.get(\"title\", \"\").strip() or a.get_text(\" \", strip=True)\n",
    "\n",
    "        if not date_text:\n",
    "            time_tag = soup.find(\"time\")\n",
    "            if time_tag:\n",
    "                date_text = time_tag.get(\"datetime\", \"\").strip() or time_tag.get_text(\" \", strip=True)\n",
    "\n",
    "        return {\n",
    "            \"Nguồn\": \"Tin quốc tế\",\n",
    "            \"Tiêu đề\": title,\n",
    "            \"Mô tả\": title,\n",
    "            \"Ngày\": date_text,\n",
    "            \"URL\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "\n",
    "    urls = get_article_links()\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        data = scrape_article(url)\n",
    "        if data:\n",
    "            articles.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    output_dir = r\"C:\\Users\\Admin\\Documents\\VIETTEL\\2-crawl-articles\\scraper\\output-scraper\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not df.empty:\n",
    "        excel_file = f\"{output_dir}/thinkbroadband_{timestamp}.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"\\nSaved results to {excel_file}\")\n",
    "    else:\n",
    "        print(\"\\nNo articles matched the keywords in URLs.\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
